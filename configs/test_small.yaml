experiment_name: "mamba_test_small"
output_dir: "outputs"
log_level: "INFO"
seed: 42

model:
  d_model: 128
  d_state: 16
  d_conv: 4
  expand: 2
  n_layers: 4
  vocab_size: 1000
  pad_token_id: 0

training:
  batch_size: 4
  gradient_accumulation_steps: 2
  learning_rate: 0.001
  weight_decay: 0.1
  max_grad_norm: 1.0
  num_epochs: 2
  warmup_steps: 10
  save_steps: 50
  eval_steps: 25
  use_mixed_precision: false
  gradient_checkpointing: false

data:
  max_seq_length: 128
  tokenizer_path: "tokenizer.model"
  dataset_path: "data/"
  num_workers: 0
  preprocessing_batch_size: 100
  train_split: 0.9
  val_split: 0.1
  shuffle_data: true